{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "#load data\n",
    "house_price_train = [[240,2],[260,2],[260,3],[360,2],[420,1],[350,2],[285,1]]\n",
    "size_train = [1200,1400,1900,2600,2700,2400,1150]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# creating: neural net for regression\n",
    "# backpropagation from scratch\n",
    "# Training on m=7 examples, 2 features\n",
    "# 1 hidden layer, 2 nodes, ReLu activation\n",
    "# MSE loss function, no regularization\n",
    "\n",
    "#randomly initialize weights for first and second layer\n",
    "weights = np.random.rand(2,3)*10\n",
    "weights2 = np.random.rand(1,3)*10\n",
    "\n",
    "#identify features and add bias unit\n",
    "feat = np.array(house_price_train)\n",
    "m = len(feat)\n",
    "feat_bias = np.column_stack([np.ones((m, 1)), feat])\n",
    "\n",
    "# initialize gradient accumulator\n",
    "big_delta = np.zeros([2,1,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Forward propagates for a given training example _i_\n",
    "def propagate(i):\n",
    "    global weights2, weights\n",
    "\n",
    "    # z for second layer and activation (ReLu)\n",
    "    z2 = np.dot(weights, np.transpose(feat_bias[i, :]))\n",
    "    a2 = np.maximum(0, z2)\n",
    "    # add bias unit for second layer\n",
    "    a2_bias = np.concatenate((np.ones(1), a2))\n",
    "\n",
    "    # z for third layer and final activation (ReLu)\n",
    "    z3 = np.dot(weights2, a2_bias)\n",
    "    hypot = np.maximum(0, z3)\n",
    "\n",
    "    print(f'Prediction: {z3}')\n",
    "\n",
    "    # Backpropagates to accumulate partials and deltas\n",
    "    # Delta for third layer (mean squared error)\n",
    "    delta3 =  size_train[i] - hypot\n",
    "    # derivative of error w.r.t weights, second layer\n",
    "    layer2partials = np.multiply(delta3, a2_bias)\n",
    "\n",
    "    # derivative of error w.r.t weights, first layer\n",
    "    # big_delta2 without bias\n",
    "    delta2 = np.multiply(delta3, a2)\n",
    "    delta2_matrix = np.column_stack([delta2, delta2, delta2])\n",
    "    featBias_matrix = np.row_stack([a2_bias, a2_bias])\n",
    "    layer1partials = np.multiply(delta2_matrix, featBias_matrix)\n",
    "\n",
    "    weights2 = weights2 - layer2partials\n",
    "    weights = weights - layer1partials"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.]\n"
     ]
    }
   ],
   "source": [
    "propagate(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [7473.4847457]\n"
     ]
    }
   ],
   "source": [
    "a = 0.01\n",
    "# z for second layer and activation (ReLu)\n",
    "z2 = np.dot(weights, np.transpose(feat_bias[1, :]))\n",
    "a2 = np.maximum(0, z2)\n",
    "# add bias unit for second layer\n",
    "a2_bias = np.concatenate((np.ones(1), a2))\n",
    "\n",
    "# z for third layer and final activation (ReLu)\n",
    "z3 = np.dot(weights2, a2_bias)\n",
    "hypot = np.maximum(0, z3)\n",
    "\n",
    "print(f'Prediction: {z3}')\n",
    "\n",
    "# Backpropagates to accumulate partials and deltas\n",
    "# Delta for third layer (mean squared error)\n",
    "delta3 =  size_train[1] - hypot\n",
    "# derivative of error w.r.t weights, second layer\n",
    "layer2partials = np.multiply(delta3, a2_bias)\n",
    "\n",
    "# derivative of error w.r.t weights, first layer\n",
    "layer2partials_no_bias = np.multiply(delta3, a2)\n",
    "delta2_matrix = np.column_stack([layer2partials_no_bias, layer2partials_no_bias, layer2partials_no_bias])\n",
    "featBias_matrix = np.row_stack([feat_bias[1,:], feat_bias[1,:]])\n",
    "layer1partials = np.multiply(delta2_matrix, featBias_matrix)\n",
    "\n",
    "weights2 = weights2 - a*layer2partials\n",
    "weights = weights - a*layer1partials"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}