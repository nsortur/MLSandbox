{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "# User-driven regression nn with one hidden layer, leaky-ReLu activation, MSE\n",
    "# on sklearn's diabetes dataset\n",
    "# adding a 2nd hidden layer\n",
    "\n",
    "num_hid_units = 8\n",
    "num_hid_units2 = 8\n",
    "num_features = 2\n",
    "a = 0.000005\n",
    "\n",
    "# house_sk_data = datasets.load_boston()\n",
    "# # size of training and cross validation sets\n",
    "# m_train = 400\n",
    "# m_cv = 506 - m_train\n",
    "\n",
    "# either dataset works\n",
    "house_sk_data = datasets.load_diabetes()\n",
    "m_train = 266\n",
    "m_cv = 133\n",
    "m_test = 43\n",
    "\n",
    "# training set\n",
    "house_sk_train = preprocessing.scale(house_sk_data.data[:m_train, :num_features])\n",
    "house_sk_train_bias = np.column_stack([np.ones((m_train, 1)), house_sk_train])\n",
    "\n",
    "# CV set\n",
    "house_sk_cv = preprocessing.scale(house_sk_data.data[m_train:(m_train+m_cv), :num_features])\n",
    "house_sk_cv_bias = np.column_stack([np.ones((m_cv, 1)), house_sk_cv])\n",
    "\n",
    "# test set\n",
    "house_sk_test = preprocessing.scale(house_sk_data.data[(m_train+m_cv):(m_train+m_cv+m_test), :num_features])\n",
    "house_sk_test_bias = np.column_stack([np.ones((m_test, 1)), house_sk_test])\n",
    "\n",
    "# target set\n",
    "house_sk_target = house_sk_data.target[:m_train]\n",
    "house_sk_cv_target = house_sk_data.target[m_train:(m_train+m_cv)]\n",
    "house_sk_test_target = house_sk_data.target[(m_train+m_cv):(m_train+m_cv+m_test)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "# randomly initialize weights for first, second, and third layers\n",
    "weights = np.random.rand(num_hid_units, num_features+1)\n",
    "weights2 = np.random.rand(num_hid_units2, num_hid_units+1)\n",
    "weights3 = np.random.rand(1, num_hid_units2+1)\n",
    "\n",
    "\n",
    "# Forward and backward propagates for a given training example _i_\n",
    "def propagate(i):\n",
    "    global weights3, weights2, weights\n",
    "\n",
    "    # learning rate\n",
    "    # 0.0003 retains lowest MSE for CV set\n",
    "\n",
    "    # z for second layer and activation (leaky-ReLu)\n",
    "    z2 = np.dot(weights, np.transpose(house_sk_train_bias[i, :]))\n",
    "    a2 = np.where(z2 > 0, z2, 0.01 * z2)\n",
    "    # add bias unit for second layer\n",
    "    a2_bias = np.concatenate((np.ones(1), a2))\n",
    "    # z for third layer and final activation (leaky-ReLu)\n",
    "    z3 = np.dot(weights2, a2_bias)\n",
    "    a3 = np.where(z3 > 0, z3, 0.01 * z3)\n",
    "    # add bias unit for third layer\n",
    "    a3_bias = np.concatenate((np.ones(1), a3))\n",
    "    # linear hypothesis\n",
    "    hypot = np.dot(weights3, a3_bias)\n",
    "    print('hypot: ', hypot)\n",
    "    # Backpropagates to accumulate partials and deltas\n",
    "    # Delta for fourth layer (mean squared error)\n",
    "    delta4 = hypot - house_sk_target[i]\n",
    "    # derivative of error w.r.t weights, third layer\n",
    "    layer3partials = np.multiply(delta4, a3_bias)\n",
    "\n",
    "    # derivative of error w.r.t weights, second layer\n",
    "    layer3partials_nobias = layer3partials[1:]\n",
    "    layer2partials = np.dot(layer3partials_nobias.reshape((-1,1)), np.transpose(a2_bias.reshape((-1,1))))\n",
    "\n",
    "    # derivative of error w.r.t weights, first layer\n",
    "    layer2partials_nobias = layer2partials[:, 1:]\n",
    "    #feat_bias_matrix = np.row_stack([house_sk_train_bias[i, :] for n in range(num_hid_units)])\n",
    "    feat_bias_matrix = house_sk_train_bias[i, :]\n",
    "    print('layer 2 partials: ', np.shape(layer2partials_nobias))\n",
    "    print('feat bias: ', np.shape(feat_bias_matrix))\n",
    "    print('weights1: ', np.shape(weights))\n",
    "    layer1partials = np.dot(layer2partials_nobias, feat_bias_matrix)\n",
    "\n",
    "    weights3 = weights3 - (a * layer3partials)\n",
    "    weights2 = weights2 - (a * layer2partials)\n",
    "    weights = weights - (a * layer1partials)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypot:  [33.06044735]\n",
      "layer 2 partials:  (8, 8)\n",
      "feat bias:  (3,)\n",
      "weights1:  (8, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (8,8) and (3,) not aligned: 8 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-162-ddcf05c254a6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Train weights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mz\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mpropagate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-161-ad2e21efafe3>\u001B[0m in \u001B[0;36mpropagate\u001B[0;34m(i)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'feat bias: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeat_bias_matrix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'weights1: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m     \u001B[0mlayer1partials\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer2partials_nobias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeat_bias_matrix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0mweights3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweights3\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mlayer3partials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (8,8) and (3,) not aligned: 8 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Train weights\n",
    "for z in range(0, m_train):\n",
    "    propagate(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict example in given set\n",
    "def predict(j, pred_set, target_set):\n",
    "    # z for second layer and activation (ReLu)\n",
    "    z2 = np.dot(weights, np.transpose(pred_set[j, :]))\n",
    "    # print('z2', z2)\n",
    "    a2 = np.where(z2 > 0, z2, 0.01 * z2)\n",
    "    # print('a2', a2)\n",
    "    # add bias unit for second layer\n",
    "    a2_bias = np.concatenate((np.ones(1), a2))\n",
    "    # z for third layer, no final activation for now\n",
    "    z3 = np.dot(weights3, a2_bias)\n",
    "\n",
    "    # calculate error\n",
    "    sq_error = (np.square(z3 - target_set[j]))\n",
    "    print(f'Prediction: {z3}, actual: {target_set[j]}, error: {0.5 * sq_error}')\n",
    "    return sq_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tot_err_train = 0\n",
    "# Predict in training example and see average error\n",
    "for y in range(0, m_train):\n",
    "    tot_err_train += predict(y, house_sk_train_bias, house_sk_target)\n",
    "\n",
    "print('=========CV-SET=========')\n",
    "tot_err = 0\n",
    "# Predict CV example and see average error\n",
    "for x in range(0, m_cv):\n",
    "    tot_err += predict(x, house_sk_cv_bias, house_sk_cv_target)\n",
    "\n",
    "print('MSE training ($): ', tot_err_train / (2 * m_train))\n",
    "print('MSE CV ($): ', tot_err / (2 * m_cv))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}